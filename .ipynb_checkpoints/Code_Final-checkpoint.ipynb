{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "acac68b7",
   "metadata": {},
   "source": [
    "# Package Import + Model Definition + Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43e72816",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "from scipy.optimize import minimize\n",
    "import time\n",
    "import pickle\n",
    "import uuid\n",
    "\n",
    "# Constants\n",
    "M_REPLICATES = 10\n",
    "LAYER_UNITS = [50, 25]\n",
    "N_ENSEMBLE = 10\n",
    "SIGMA = 0.25\n",
    "\n",
    "def build_model(layer_units, dropout_rate):\n",
    "    \"\"\"Construct neural network with named layers for clarity\"\"\"\n",
    "    tf.keras.backend.clear_session()\n",
    "    \n",
    "    model = Sequential(name=\"Sequential_Model\")\n",
    "    \n",
    "    # Input layer with explicit name\n",
    "    model.add(Input(shape=(5,), name=\"Input_Layer\"))\n",
    "    \n",
    "    # Hidden layers with numbered names\n",
    "    for i, units in enumerate(layer_units, start=1):\n",
    "        model.add(\n",
    "            Dense(units, activation='tanh', name=f\"Hidden_Tanh_{i}\")\n",
    "        )\n",
    "    \n",
    "    # Dropout layer with descriptive name\n",
    "    model.add(\n",
    "        Dropout(dropout_rate, name=f\"Dropout_{dropout_rate:.2f}\")\n",
    "    )\n",
    "    \n",
    "    # Output layer\n",
    "    model.add(Dense(1, name=\"Output_Layer\"))\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    \n",
    "    # Verify names (optional debug print)\n",
    "    print(\"Layer names:\", [layer.name for layer in model.layers])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def generate_data(random_seed, SIGMA = 0.25):\n",
    "    \"\"\"Generate synthetic dataset with proper seeding\"\"\"\n",
    "    np.random.seed(random_seed)\n",
    "    n_samples = 2000\n",
    "    X = np.random.uniform(0, 10, size=(n_samples, 1))\n",
    "    noise = np.random.normal(0, SIGMA, n_samples)\n",
    "    y = np.sin(X[:, 0]) + noise\n",
    "    X_poly = np.hstack([X, X**2, X**3, X**4, X**5])\n",
    "    X_poly = (X_poly - np.mean(X_poly, axis = 0)) / np.std(X_poly, axis=0)\n",
    "    return X_poly, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa99d58",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7963861",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc_dropout_predictions(model, X, n_samples=100):\n",
    "    preds = np.array([model(X, training=True).numpy().squeeze() for _ in range(n_samples)])\n",
    "    return preds.mean(0), preds.std(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f709b18",
   "metadata": {},
   "source": [
    "# Ensemble Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca5d0f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_ensemble(model, X_train, y_train, X_test, X_val, ensemble_type, n_ensemble=10, verb=1):\n",
    "        preds, val_preds, train_times, infer_times = [], [], [], []\n",
    "\n",
    "        for _ in range(n_ensemble):\n",
    "            # Training setup\n",
    "            start_train = time.time()\n",
    "            if ensemble_type == 'bootstrap':\n",
    "                idx = np.random.choice(len(X_train), len(X_train), replace=True)\n",
    "                model.fit(X_train[idx], y_train[idx], epochs=100, verbose=verb)\n",
    "            elif ensemble_type == 'bayesian':\n",
    "                weights = np.random.dirichlet(np.ones(len(X_train)))\n",
    "                model.fit(X_train, y_train, sample_weight=weights, epochs=100, verbose=verb)\n",
    "            elif ensemble_type == 'parametric':\n",
    "                y_synth = model.predict(X_train) + np.random.normal(0, np.std(y_train - model.predict(X_train)), y_train.shape)\n",
    "                model.fit(X_train, y_synth, epochs=100, verbose=verb)\n",
    "\n",
    "            train_times.append(time.time() - start_train)\n",
    "\n",
    "            # Inference\n",
    "            start_infer = time.time()\n",
    "            preds.append(model.predict(X_test).squeeze())\n",
    "            val_preds.append(model.predict(X_val).squeeze())\n",
    "            infer_times.append(time.time() - start_infer)\n",
    "\n",
    "        return np.array(preds), np.array(val_preds), train_times, infer_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d829c8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(x, y_true, pred_mean, pred_std, title, filename):\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.scatter(x, y_true, label='Simulated Data', color='black', alpha=0.6, s=10)\n",
    "    plt.plot(x, pred_mean, label='Prediction Mean', linewidth=2)\n",
    "    plt.fill_between(x, pred_mean - pred_std, pred_mean + pred_std, alpha=0.3, label='Uncertainty')\n",
    "    plt.title(title)\n",
    "    plt.xlabel('X')\n",
    "    plt.ylabel('Y')\n",
    "    plt.legend()\n",
    "    plt.savefig(filename)\n",
    "    plt.close()\n",
    "    \n",
    "def plot_combined_comparison(X_test, y_test, mc_mean, mc_std, results, filename):\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sort_idx = X_test[:, 0].argsort()\n",
    "    X_sorted = X_test[sort_idx, 0].squeeze()\n",
    "    y_sorted = y_test[sort_idx]\n",
    "\n",
    "    # Plot ground truth\n",
    "    plt.scatter(X_sorted, y_sorted, label='Observed Data',\n",
    "                color='black', alpha=0.4, s=15, zorder=5)\n",
    "\n",
    "    # Plot MC Dropout\n",
    "    plt.plot(X_sorted, mc_mean[sort_idx], label='MC Dropout',\n",
    "             color='#1f77b4', linewidth=2.5)\n",
    "    plt.fill_between(X_sorted,\n",
    "                     mc_mean[sort_idx] - mc_std[sort_idx],\n",
    "                     mc_mean[sort_idx] + mc_std[sort_idx],\n",
    "                     color='#1f77b4', alpha=0.15)\n",
    "\n",
    "    # Plot ensembles\n",
    "    colors = ['#ff7f0e', '#2ca02c', '#d62728']\n",
    "    styles = ['--', '-.', ':']\n",
    "    for (method, data), color, style in zip(results.items(), colors, styles):\n",
    "        label = method.title().replace('_', ' ') + ' Ensemble'\n",
    "        plt.plot(X_sorted, data['mean'][sort_idx], label=label,\n",
    "                 linestyle=style, linewidth=2, color=color)\n",
    "        plt.fill_between(X_sorted,\n",
    "                         data['mean'][sort_idx] - data['std'][sort_idx],\n",
    "                         data['mean'][sort_idx] + data['std'][sort_idx],\n",
    "                         color=color, alpha=0.15)\n",
    "\n",
    "    plt.xlabel('Input Feature (X)', fontsize=12)\n",
    "    plt.ylabel('Target Value (Y)', fontsize=12)\n",
    "    plt.title('Combined Uncertainty Quantification Comparison', fontsize=14)\n",
    "    plt.legend(fontsize=10, loc='upper right')\n",
    "    plt.grid(alpha=0.2)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(filename, dpi=300)\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3cf6d6c",
   "metadata": {},
   "source": [
    "# Result Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c9ec8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bars_with_bands(grand_mean, grand_meansd, sd_of_sd, sort_idx, X_test, filename='test', title='hi mom', direc = \".\"):\n",
    "    os.makedirs(direc, exist_ok=True)  # Ensure directory exists\n",
    "    os.chdir(direc)  # Use the provided `direc` argument\n",
    "    step = 20\n",
    "    selected_x = X_test[sort_idx][::step]\n",
    "    y_mean = grand_mean[sort_idx] + 1.96 * grand_meansd[sort_idx]\n",
    "    selected_y = y_mean[::step]\n",
    "    selected_std = 1.96 * sd_of_sd[::step]\n",
    "    \n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.plot( X_test[sort_idx], grand_mean[sort_idx])\n",
    "    plt.plot( X_test[sort_idx], \n",
    "                grand_mean[sort_idx] - 1.96 * grand_meansd[sort_idx], \n",
    "                alpha = 0.25, ls='--')\n",
    "    plt.plot( X_test[sort_idx], \n",
    "                grand_mean[sort_idx] + 1.96 * grand_meansd[sort_idx], \n",
    "                alpha = 0.25, ls='--')\n",
    "    plt.errorbar(\n",
    "        selected_x, \n",
    "        selected_y, \n",
    "        yerr= selected_std, \n",
    "        fmt='s',          # Square markers\n",
    "        color='green', \n",
    "        ecolor='orange', \n",
    "        capsize=5,\n",
    "        label='Error Bars (Every 20th point)'\n",
    "    )\n",
    "    plt.title(title)\n",
    "    plt.savefig(filename + '.png')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbccfcd6",
   "metadata": {},
   "source": [
    "# Body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3042f0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(LAYER_UNITS = [50, 25], DROPOUT_RATE = 0.1, N_ENSEMBLE = 5, N_MCD = 100):\n",
    "    X2, y = generate_data(42)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X2, y, test_size=0.2, random_state=1)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=1)\n",
    "    \n",
    "    base_model = build_model(LAYER_UNITS, DROPOUT_RATE)\n",
    "    base_model.fit(X_train, y_train, epochs=100, batch_size=32,\n",
    "                      verbose=0, validation_data=(X_val, y_val))\n",
    "    \n",
    "    # MC Dropout\n",
    "    start_time = time.time()\n",
    "    mc_preds = mc_dropout_predictions(base_model, X_test)\n",
    "    mc_total_infer_time = time.time() - start_time\n",
    "    mc_mean = mc_preds[0]\n",
    "    mc_std = mc_preds[1]\n",
    "    # Bootstrap Ensemble \n",
    "\n",
    "    ensembles = {\n",
    "        'bootstrap': run_ensemble(base_model, X_train, y_train, X_test, X_val, 'bootstrap', n_ensemble=N_ENSEMBLE, verb=0),\n",
    "        'bayesian': run_ensemble(base_model, X_train, y_train, X_test, X_val, 'bayesian', n_ensemble=N_ENSEMBLE, verb=0),\n",
    "        'parametric': run_ensemble(base_model, X_train, y_train, X_test, X_val, 'parametric', n_ensemble=N_ENSEMBLE, verb=0)\n",
    "    }\n",
    "\n",
    "\n",
    "    # Process results\n",
    "    results = {}\n",
    "    for name, (preds, val_preds, train_t, infer_t) in ensembles.items():\n",
    "        # Calculate metrics\n",
    "        val_mean = val_preds.mean(0)\n",
    "        val_std_uc = val_preds.std(0)\n",
    "        val_std_uc[val_std_uc == 0] = 1e-10  # or some other small value\n",
    "        residuals = y_val - val_mean\n",
    "\n",
    "        # Calibration optimization\n",
    "        def loss(params):\n",
    "            a, b = params\n",
    "            sigma_cal = a * val_std_uc + b\n",
    "            sigma_cal = np.clip(sigma_cal, 1e-6, None)\n",
    "            return np.sum(0.5 * (residuals**2 / sigma_cal**2) + np.log(sigma_cal))\n",
    "\n",
    "        opt = minimize(loss, [1, 0], method='Nelder-Mead')\n",
    "        a, b = opt.x\n",
    "\n",
    "        # Calculate calibrated uncertainties\n",
    "        val_std_cal = a * val_std_uc + b\n",
    "        test_std_uc = preds.std(0)\n",
    "        test_std_cal = a * test_std_uc + b\n",
    "\n",
    "        # Generate calibration plots\n",
    "        plt.figure(figsize=(12, 5))\n",
    "\n",
    "        # Uncalibrated Q-Q plot\n",
    "        plt.subplot(121)\n",
    "        stats.probplot(residuals/val_std_uc, dist=\"norm\", plot=plt)\n",
    "        plt.title(f'{name.title()} Uncalibrated Residuals')\n",
    "\n",
    "        # Calibrated Q-Q plot\n",
    "        plt.subplot(122)\n",
    "        stats.probplot(residuals/val_std_cal, dist=\"norm\", plot=plt)\n",
    "        plt.title(f'{name.title()} Calibrated Residuals')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{name}_calibration.png')\n",
    "        plt.close()\n",
    "\n",
    "        # Store results\n",
    "        results[name] = {\n",
    "                'mean': preds.mean(0),\n",
    "                'std': test_std_cal,\n",
    "                'metrics': {\n",
    "                    'MSE': mean_squared_error(y_test, preds.mean(0)),\n",
    "                    'MAE': mean_absolute_error(y_test, preds.mean(0)),\n",
    "                    'Avg Uncertainty': test_std_cal.mean()\n",
    "                },\n",
    "                'times': {\n",
    "                    'train': np.mean(train_t),\n",
    "                    'infer': np.sum(infer_t)\n",
    "                }\n",
    "            }\n",
    "        # Save prediction plots\n",
    "        # sort_idx = X_test[:, 0].argsort()\n",
    "        # plot_results(X_test[sort_idx, 0], y_test[sort_idx],\n",
    "        #             preds.mean(0)[sort_idx], test_std_cal[sort_idx],\n",
    "        #             f'{name.title()} Ensemble Predictions',\n",
    "        #             f'{name}_ensemble.png')\n",
    "\n",
    "        # Generate combined comparison plot\n",
    "        # plot_combined_comparison(\n",
    "        #    X_test, y_test,\n",
    "        #    mc_mean, mc_std,\n",
    "        #    results,\n",
    "        #    f'combined_methods_comparison.png'\n",
    "        # )\n",
    "        \n",
    "    # Save metrics\n",
    "    metrics_df = pd.DataFrame({k: v['metrics'] for k, v in results.items()}).T\n",
    "    # metrics_df.to_csv('metrics_comparison.csv')\n",
    "\n",
    "    comp_data = {\n",
    "    \"MC Dropout\": {\n",
    "        \"Avg Training Time (s)\": None,\n",
    "        \"Total Inference Time (s)\": round(mc_total_infer_time, 4),\n",
    "        \"Avg Inference Time (s)\": round(mc_total_infer_time/100, 6)\n",
    "        }\n",
    "    }\n",
    "\n",
    "    for name, data in results.items():\n",
    "        comp_data[name.title() + \" Ensemble\"] = {\n",
    "              \"Avg Training Time (s)\": round(data['times']['train'], 4),\n",
    "              \"Total Inference Time (s)\": round(data['times']['infer'], 4),\n",
    "              \"Avg Inference Time (s)\": round(np.mean(data['times']['infer']/N_ENSEMBLE), 6)\n",
    "          }\n",
    "\n",
    "        comp_df = pd.DataFrame(comp_data).T\n",
    "        comp_df = comp_df[[\n",
    "          \"Avg Training Time (s)\",\n",
    "          \"Total Inference Time (s)\",\n",
    "          \"Avg Inference Time (s)\"\n",
    "        ]]\n",
    "    # comp_df.to_csv('computational_metrics.csv')\n",
    "    \n",
    "    ret = dict()\n",
    "    ret['mc_dropout_mean'] = mc_mean\n",
    "    ret['mc_dropout_sd']   = mc_std\n",
    "    ret['mc_total_infer_time'] = mc_total_infer_time\n",
    "    ret['ensembles']       = ensembles\n",
    "    ret['metrics_df']      = metrics_df\n",
    "    ret['comp_df']         = comp_df\n",
    "    ret['calibration_results'] = results\n",
    "    ret['dataset'] = [X_train, y_train, X_test, y_test, X_val, y_val]\n",
    "    \n",
    "\n",
    "    # Generate a random filename (e.g., \"4a3b2c1d.pkl\")\n",
    "    filename = f\"{uuid.uuid4().hex}.pkl\"\n",
    "\n",
    "    # Save the dictionary\n",
    "    with open(filename, \"wb\") as f:\n",
    "        pickle.dump(ret, f)\n",
    "\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "21938faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def doVis(ret,DROPOUT_RATE=0.1):\n",
    "    res = ret\n",
    "    sort_idx = res[0]['dataset'][2][:,0].argsort()\n",
    "    mc_dropout_grand_mean = np.mean([res[k]['mc_dropout_mean'] for k in range(len(res))], axis = 0)\n",
    "    mc_dropout_grand_meansd = np.mean([res[k]['mc_dropout_sd'] for k in range(len(res))], axis = 0)\n",
    "    mc_dropout_sd_of_sd = np.std([res[k]['mc_dropout_sd'] for k in range(len(res))], axis = 0)\n",
    "    #   M   ensembles    method  test_pred\n",
    "    # mean curve\n",
    "    # sd for bands\n",
    "    # sd for bars on the bands\n",
    "    print(\"Hey :p\")\n",
    "    bootstrap_grand_mean = np.mean([res[k]['calibration_results']['bootstrap']['mean'] for k in range(len(res))], axis = 0)\n",
    "    bootstrap_grand_meansd = np.mean([res[k]['calibration_results']['bootstrap']['std'] for k in range(len(res))], axis = 0) # preds (test_preds), val_preds, train_time, inf_time \n",
    "    bootstrap_sd_of_sd = np.std([res[k]['calibration_results']['bootstrap']['std'] for k in range(len(res))], axis = 0) # preds (test_preds), val_preds, train_time, inf_time \n",
    "    bayesian_grand_mean = np.mean([res[k]['calibration_results']['bayesian']['mean'] for k in range(len(res))], axis=0)\n",
    "    bayesian_grand_meansd = np.mean([res[k]['calibration_results']['bayesian']['std'] for k in range(len(res))], axis=0)  # preds (test_preds), val_preds, train_time, inf_time \n",
    "    bayesian_sd_of_sd = np.std([res[k]['calibration_results']['bayesian']['std'] for k in range(len(res))], axis=0)  # preds (test_preds), val_preds, train_time, inf_time\n",
    "    parametric_grand_mean = np.mean([res[k]['calibration_results']['parametric']['mean'] for k in range(len(res))], axis=0)\n",
    "    parametric_grand_meansd = np.mean([res[k]['calibration_results']['parametric']['std'] for k in range(len(res))], axis=0)  # preds (test_preds), val_preds, train_time, inf_time \n",
    "    parametric_sd_of_sd = np.std([res[k]['calibration_results']['parametric']['std'] for k in range(len(res))], axis=0)  # preds (test_preds), val_preds, train_time, inf_time\n",
    "    \n",
    "    bars_with_bands(mc_dropout_grand_mean, mc_dropout_grand_meansd, mc_dropout_sd_of_sd, sort_idx, res[0]['dataset'][2][:,0],\n",
    "               filename = 'MCD_DR_' + str(DROPOUT_RATE) + '_BWB',\n",
    "               title = \"MC Dropout Simulation Results (M = 10 MC Replicates)\")\n",
    "    bars_with_bands(bootstrap_grand_mean, bootstrap_grand_meansd, bootstrap_sd_of_sd, sort_idx, res[0]['dataset'][2][:,0],\n",
    "                   filename = 'BOOT_DR_' + str(DROPOUT_RATE) + '_BWB',\n",
    "                   title = \"Bootstrap BE Simulation Results (M = 10 MC Replicates)\")\n",
    "    bars_with_bands(bayesian_grand_mean, bayesian_grand_meansd, bayesian_sd_of_sd, sort_idx, res[0]['dataset'][2][:,0],\n",
    "                   filename = 'BAYES_DR_' + str(DROPOUT_RATE) + '_BWB',\n",
    "                   title = \"Bayesian BE Simulation Results (M = 10 MC Replicates)\")\n",
    "    bars_with_bands(parametric_grand_mean, parametric_grand_meansd, parametric_sd_of_sd, sort_idx, res[0]['dataset'][2][:,0],\n",
    "                   filename = 'PARAM_DR_' + str(DROPOUT_RATE) + '_BWB',\n",
    "                   title = \"Parametric BE Simulation Results (M = 10 MC Replicates)\")\n",
    "    \n",
    "    error_df = pd.concat([res[k]['metrics_df'] for k in range(len(res))]).groupby(level=0).mean()\n",
    "    comp_df = pd.concat([res[k]['comp_df'] for k in range(len(res))]).groupby(level=0).mean()\n",
    "    \n",
    "    error_df.to_csv('metrics_comparison_DR' + str(DROPOUT_RATE) + '.csv')\n",
    "    comp_df.to_csv('computational_comparison_DR' + str(DROPOUT_RATE) + '.csv')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7fa85cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ProcessPoolExecutor\n",
    "\n",
    "def pkl_process():\n",
    "    DROPOUT_RATES = [0.1, 0.5, 0.9]\n",
    "    for dr in DROPOUT_RATES:\n",
    "        BASE_DIR = \"sim_results_\" + str(dr) \n",
    "        os.makedirs(os.path.abspath(BASE_DIR), exist_ok=True)\n",
    "        os.chdir(BASE_DIR)\n",
    "        pkl_files = glob.glob(f\"{BASE_DIR}/*.pkl\")\n",
    "\n",
    "        # Load all .pkl files into a list\n",
    "        loaded_data = []\n",
    "        for file in pkl_files:\n",
    "            with open(file, \"rb\") as f:\n",
    "                data = pickle.load(f)\n",
    "                loaded_data.append(data)\n",
    "        print(loaded_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0fc734f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "import time\n",
    "import logging\n",
    "\n",
    "# Configure logging to show thread/process info\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s [%(threadName)s] - %(message)s'\n",
    ")\n",
    "\n",
    "def main2(task_id, DR):\n",
    "    \"\"\"Example task that simulates work\"\"\"\n",
    "    logging.info(f\"Task {task_id} started\")\n",
    "    logging.info(f\"CWD: {os.getcwd()}\")\n",
    "    result = main(DROPOUT_RATE=DR)\n",
    "    logging.info(\"Did the visualization\")\n",
    "    return result\n",
    "\n",
    "def wrapper2(DR = 0.1, M=10):\n",
    "    \"\"\"Parallel executor with progress tracking\"\"\"\n",
    "    results = []\n",
    "    start_time = time.time()\n",
    "    \n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:\n",
    "        # Submit tasks\n",
    "        futures = {executor.submit(main2, i, DR): i for i in range(M)}\n",
    "        \n",
    "        # Track progress as tasks complete\n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            task_id = futures[future]\n",
    "            try:\n",
    "                logging.info(f\"✅ Task {task_id} result collected\")\n",
    "            except Exception as e:\n",
    "                logging.error(f\"❌ Task {task_id} failed: {e}\")\n",
    "    logging.info(f\"All tasks done in {time.time() - start_time:.2f}s\")\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "11116595",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_sim():\n",
    "    DROPOUT_RATES = [0.1, 0.5, 0.9]\n",
    "    for dr in DROPOUT_RATES:\n",
    "        BASE_DIR = \"sim_results_\" + str(dr) \n",
    "        os.makedirs(os.path.abspath(BASE_DIR), exist_ok=True)\n",
    "        os.chdir(BASE_DIR)\n",
    "        wrapper2(DR = dr)\n",
    "        os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "78812a55",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'glob' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [11]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpickle\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Get all .pkl files in current directory\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m pkl_files \u001b[38;5;241m=\u001b[39m \u001b[43mglob\u001b[49m\u001b[38;5;241m.\u001b[39mglob(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./sim_results_0.1/*.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Load all files into a list\u001b[39;00m\n\u001b[0;32m      8\u001b[0m loaded_data \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[1;31mNameError\u001b[0m: name 'glob' is not defined"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pickle\n",
    "import glob\n",
    "\n",
    "# Get all .pkl files in current directory\n",
    "pkl_files = glob.glob(\"./sim_results_0.1/*.pkl\")\n",
    "\n",
    "# Load all files into a list\n",
    "loaded_data = []\n",
    "for file in pkl_files:\n",
    "    with open(file, \"rb\") as f:\n",
    "        loaded_data.append(pickle.load(f))\n",
    "\n",
    "os.chdir(\"./sim_results_0.1/\")\n",
    "doVis(loaded_data)\n",
    "os.chdir(\"..\")\n",
    "\n",
    "os.chdir(\"./sim_results_0.5/\")\n",
    "doVis(loaded_data)\n",
    "os.chdir(\"..\")\n",
    "\n",
    "os.chdir(\"./sim_results_0.9/\")\n",
    "doVis(loaded_data)\n",
    "os.chdir(\"..\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
